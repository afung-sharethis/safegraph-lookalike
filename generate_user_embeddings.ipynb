{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating User Embeddings\n",
    "This notebook generates user embeddings for use in the Safegraph Lookalike model.\n",
    "\n",
    "To use this notebook, edit the variables in the `Loading the data` section below.\n",
    "\n",
    "#### Expected label-dataset columns:\n",
    "- `titles`: A string of aggregated webpage titles visited by the user\n",
    "- `keywords`: A string of aggregated keywords from webpages visited by the user\n",
    "- `domains`: A string of aggregated domains of the webpages visited by the user\n",
    "\n",
    "#### Outputs (see results section below):\n",
    "- 2-Dimensional plot: `plotly_results/{file_name}_{embedding_type}.html`\n",
    "- NPY pickle of generated embeddings: `npy_pickles/{file_name}_{embedding_type}.npy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TFHUB_CACHE_DIR=models\n",
    "%env CUDA_VISIBLE_DEVICES=\"\"  # Comment out to allow GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tldextract as tld\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Embeddings Models\n",
    "import tensorflow_text\n",
    "import tensorflow_hub as hub\n",
    "import fasttext\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_domains(domains):\n",
    "    \"\"\" Extracts only the primary domain for each domain in the list  \"\"\"\n",
    "    return ', '.join([tld.extract(s).domain for s in domains.split(',')])\n",
    "\n",
    "def parse_keywords(keywords):\n",
    "    \"\"\" Replace '_' characters with a space \"\"\"\n",
    "    return keywords.replace('_', ' ').replace(',', ', ')\n",
    "\n",
    "def parse_titles(titles):\n",
    "    \"\"\" Replace '|' characters with a comma \"\"\"\n",
    "    return titles.replace('|', ', ')\n",
    "\n",
    "def process(df, group_name):\n",
    "    \"\"\" Apply all pre-processing filters to dataframe \"\"\"\n",
    "    df['domains'] = df['domains'].apply(parse_domains)\n",
    "    df['keywords'] = df['keywords'].apply(parse_keywords)\n",
    "    df['titles'] = df['titles'].apply(parse_titles)\n",
    "    df['group'] = group_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(text_sets, batch_size=300):\n",
    "    \"\"\" Return a batch of sentences from a set of text series \"\"\"\n",
    "    for texts in text_sets:\n",
    "        idx = 0\n",
    "        t_len = len(texts)\n",
    "        while idx < t_len:\n",
    "            yield texts[idx:min(idx+batch_size, t_len)]\n",
    "            idx += batch_size\n",
    "\n",
    "def gen_embeddings(emb_type, df_list):\n",
    "    \"\"\" Generates embeddings from all rows of all DF's in the input list\n",
    "    \n",
    "    Valid Embedding Types:\n",
    "        - 'USE': Generates English-only USE-4 sentence embeddings\n",
    "        - 'MUSE': Generates Multilingual USE-3 sentence embeddings\n",
    "        - 'SBERT': Generates English-only SBERT sentence embeddings\n",
    "        - 'SBERT-POST': Generates post-trained English-only SBERT sentence embeddings\n",
    "        - 'FASTTEXT': Generates English-only FastText composite word embeddings\n",
    "        \n",
    "    Development:\n",
    "        Adding or changing models is trivial; simply extend the if-chain. Another\n",
    "        viable option is to use a dictionary mapping each key to a function/lambda.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Model inputs are concatenated keywords and titles, this is a list \n",
    "    # of pandas series. Each series is the text for a specific input DF\n",
    "    text = [df[['keywords', 'titles']].agg(' '.join, axis=1) for df in df_list]\n",
    "    \n",
    "    # Generate based on model type, we instantiate the model on-demand \n",
    "    # to save GPU memory.\n",
    "    # TODO: use locally saved models, and perhaps store embeddings in a\n",
    "    #       better-fitting data-structure\n",
    "    if emb_type=='USE':\n",
    "        model_use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "        return [model_use(s) for s in batch(text)]\n",
    "    elif emb_type=='MUSE':\n",
    "        # Did not run this to completion yet, but code \"compiles\"\n",
    "        model_use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
    "        return [model_use(s) for s in batch(text)]\n",
    "    elif emb_type=='SBERT':\n",
    "        model_sbert = SentenceTransformer('bert-base-nli-mean-tokens') \n",
    "        return [model_sbert.encode(s) for s in batch(text)]\n",
    "    elif emb_type=='SBERT-POST':\n",
    "        model_sbert = SentenceTransformer('output/training_stsbenchmark_continue_training-bert-base-nli-mean-tokens-2020-08-21_13-47-18/')\n",
    "        return [model_sbert.encode(s) for s in batch(text)]\n",
    "    elif emb_type=='FASTTEXT':\n",
    "        # Did not get FastText working, type error on the inputs.\n",
    "        model_fast = fasttext.load_model('models/cc.en.300.bin')\n",
    "        return [model_fast[s] for s in text]\n",
    "    else:\n",
    "        raise KeyError(\"Unrecognized embedding type: {0}\".format(emb_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(embeddings, reduce_type='tSNE'):\n",
    "    \"\"\" Reduces input embeddings to 2 dimensions for visualization \n",
    "    \n",
    "    Valid Reduction Types:\n",
    "        - 'tSNE'\n",
    "        - 'PCA'\n",
    "        - 'UMAP'\n",
    "        \n",
    "    Development:\n",
    "        Adding or changing reduction methods is trivial; simply\n",
    "        extend the if-chain\n",
    "    \"\"\"\n",
    "    # Define dimensionality reduction algorithm\n",
    "    alg = PCA(n_components=2) if reduce_type == 'PCA' else \\\n",
    "          TSNE(n_components=2) if reduce_type == 'tSNE' else \\\n",
    "          UMAP() if reduce_type == 'UMAP' else None\n",
    "    if alg is None:\n",
    "        raise KeyError(\"Unrecognized reduction type: {0}\".format(reduce_type))\n",
    "\n",
    "    # Execute reduction\n",
    "    principalComponents = alg.fit_transform(embeddings)\n",
    "    principalDf = pd.DataFrame(data=principalComponents, \n",
    "                               columns=['principal component 1', 'principal component 2'])\n",
    "    return principalDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_labels(df, label_dfs, char_lim=200):\n",
    "    \"\"\" Attatch domain, title, and group labels to an embedding DF \"\"\"\n",
    "    replace_endl = lambda s: s.replace('\\n', '<br> ')\n",
    "    domains = pd.concat([df_i['domains'] for df_i in label_dfs], ignore_index=True)\n",
    "    titles = pd.concat([df_i['titles'] for df_i in label_dfs], ignore_index=True)\n",
    "    groups = pd.concat([df_i['group'] for df_i in label_dfs], ignore_index=True)\n",
    "    keywords = pd.concat([df_i['keywords'] for df_i in label_dfs], ignore_index=True)\n",
    "    \n",
    "    # Clip and wrap labels for hover text\n",
    "    hover = \"<br><br>Groups: \" + groups.astype(str).str[:char_lim].str.wrap(75).apply(replace_endl) + \\\n",
    "        \"<br><br>Domains: \" + domains.astype(str).str[:char_lim].str.wrap(75).apply(replace_endl) + \\\n",
    "        \"<br><br>Keywords: \" + keywords.astype(str).str[:char_lim].str.wrap(75).apply(replace_endl) + \\\n",
    "        \"<br><br>Titles: \" + titles.astype(str).str[:char_lim].str.wrap(75).apply(replace_endl)\n",
    "    \n",
    "    # If memory becomes an issue, add only the hover text and no other label\n",
    "    new_df = pd.concat([df, domains, titles, groups, hover], axis = 1)\n",
    "    new_df.columns = ['principal component 1', 'principal component 2', 'domains', 'titles', 'group', 'hover']\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "This is usually the only cell you will need to edit. Simply change the variables within this cell to fit your job, and then run the notebook.\n",
    "\n",
    "#### Outputs\n",
    "- 2-Dimensional plot: `plotly_results/{file_name}_{embedding_type}.html`\n",
    "- NPY pickle of generated embeddings: `npy_pickles/{file_name}_{embedding_type}.npy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = 2000\n",
    "full_dir=\"data/safegraph/merge_brands/safegraph_sephora.csv\"\n",
    "\n",
    "file_name = \"sephora\"\n",
    "embedding_type = 'SBERT-POST'\n",
    "reduce_type = 'tSNE'\n",
    "\n",
    "ful_df = pd.read_csv(full_dir).dropna().head(num_embeddings)\n",
    "process(ful_df, 'sephora')\n",
    "\n",
    "df_list = [ful_df]\n",
    "total_embeddings = sum(df_i.shape[0] for df_i in df_list)\n",
    "print(\"Loaded {0} total embeddings\".format(total_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings\n",
    "This part will take the longest to run, as the models will have trouble with long text-inputs. Thus, we use the `capture` magic command to run the cell in the background so that the developer can close the browser safely, and come back to view the results later.\n",
    "\n",
    "**Suggested Solution:** we should limit the input size and select only the \"most significant\" text for a user, whatever that will mean.\n",
    "\n",
    "#### Output from previous runs on Beast2:\n",
    "- **USE game_seph:**                   Took 13.070s to generate 600 total embeddings\n",
    "- **SBERT game_speh:**                 Took 50.435s to generate 600 total embeddings\n",
    "- **USE all_brands:**                  Took 128.175s to generate 951 total embeddings\n",
    "- **SBERT all_brands:**                Took 1204.685s to generate 951 total embeddings\n",
    "- **SBERT-POST all_brands_ndomain10:** Took 10921.594s to generate 938 total embeddings\n",
    "- **SBERT-POST spehora:**              Took 292.435s to generate 2000 total embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture emb_output\n",
    "# ^ Run this cell in the background so it doesn't quit on it's own\n",
    "\n",
    "start = time()\n",
    "\n",
    "# Currently converting a python list of arrays to a \n",
    "# single numpy array, perhaps there is a better way\n",
    "embeddings = np.vstack(gen_embeddings(embedding_type, df_list))\n",
    "\n",
    "print('Took {:.3f}s to generate {} total embeddings'.format(time()-start, total_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the above cell is completed, print the output\n",
    "emb_output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce and Label Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "re_df = reduce(embeddings, reduce_type=reduce_type)\n",
    "\n",
    "print('Took {:.3f}s to reduce {} total embeddings'.format(time()-start, total_embeddings))\n",
    "\n",
    "finalDf = concat_labels(re_df, df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Use plotly to render results and examine each point for in-depth analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = '2 component {} on {} URLs'.format(reduce_type, num_embeddings)\n",
    "fig = px.scatter(finalDf, x=\"principal component 1\", y=\"principal component 2\", \n",
    "                 color=\"group\", hover_data=['hover'], title=title,\n",
    "                 width=1000, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Results\n",
    "Save the above plot and the generated embeddings for later analysis and use in the Lookalike Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"{0}_{1}\".format(file_name, embedding_type)\n",
    "fig.write_html(\"plotly_results/{0}.html\".format(fn))\n",
    "np.save(\"npy_pickles/{0}.npy\".format(fn), embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}